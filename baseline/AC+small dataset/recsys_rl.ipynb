{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBbEvG1aixZo"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_ranger import Ranger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"/Users/xuxiaoan/Downloads/recsys-rl-master\")\n",
    "from utils import (EvalDataset, OUNoise, Prioritized_Buffer, get_beta, \n",
    "                   preprocess_data, to_np, hit_metric, dcg_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7YiOW9TixZs"
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "rating = \"ratings.csv\"\n",
    "\n",
    "params = {\n",
    "    'max_epoch': 1,\n",
    "    'batch_size': 64,\n",
    "    'embedding_dim': 8,\n",
    "    'hidden_dim': 16,\n",
    "    'N': 5, # memory size for state_repr\n",
    "    'ou_noise':False,\n",
    "    'value_lr': 1e-4,\n",
    "    'value_decay': 1e-4,\n",
    "    'policy_lr': 1e-4,\n",
    "    'policy_decay': 1e-6,\n",
    "    'state_repr_lr': 1e-4,\n",
    "    'state_repr_decay': 1e-3,\n",
    "    'log_dir': 'logs/final/',\n",
    "    'gamma': 0.8,\n",
    "    'min_value': -10,\n",
    "    'max_value': 10,\n",
    "    'soft_tau': 1e-3,\n",
    "    'buffer_size': 100000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional recommendation task can be treated as sequental desicion making problem.\n",
    "Recommender (i.e. agent) interacts with users (i.e. environment) to sequentally suggest set of items.\n",
    "The goal is to maximize clients' satisfaction (i.e. reward).\n",
    "More specifically:\n",
    "- State is a vector $a \\in R^{3\\cdot embedding\\_dim}$ computed using the user embedding and the embeddings of `N` latest positive interactions. In the code (replay buffer) state is represented  by `(user, memory)`\n",
    "- Action is a vector $a \\in R^{embedding\\_dim}$. To get ranking score we took dot product of\n",
    "the action and the item embedding (similar to word2vec and other embedding models).\n",
    "- Reward is taken from user-item matrix (1 if rating > 3, 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning can help recommendation at least in 2 ways.\n",
    "1. User’s preference on previous items will affect his choice on the next items. \n",
    "User tends to give a higher rating if he has consecutively received more satisfied items (and vice versa). \n",
    "So, it would be more reasonable to model the recommendation as a sequential decision making process.\n",
    "2. It is important to use long-term planning in recommendations. For example, after reading the weather forecast, the user is not willing\n",
    "to read similar news. On the other hand, after watching funny videos or reading memes the user can constanly do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "qabQkULCixZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip loading data/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "# Movielens (1M) data from the https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "file_path = os.path.join(data_dir, rating)\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Skip loading \" + file_path)\n",
    "else:\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_path)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + rating)\n",
    "        tf.write(r.content)\n",
    "        \n",
    "(train_data, train_matrix, valid_data, valid_matrix, test_data, test_matrix,\n",
    " user_num, item_num, appropriate_users, mapping, reverse_mapping) = preprocess_data(data_dir, rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observation space**. As mentioned before, to get state we need `N` latest positive items (`memory`) and embedding of user. `State_Repr_Module` transform it to the vector of dimensionality `embedding_dim * 3`.\n",
    "\n",
    "- **Action space**. For every user we sample nonrelated items (the same count as related). All `available_items` which wasn't viewed before form action space.\n",
    "\n",
    "Given a state we get action embedding, compute dot product between this embedding and embeddings of all items in action space, take 1 top ranked item, compute reward, update `viewed_items` and memory, and store transition in buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.matrix = user_item_matrix\n",
    "        self.item_count = item_num\n",
    "        self.memory = np.ones([user_num, params['N']]) * item_num\n",
    "        # memory is initialized as [item_num] * N for each user\n",
    "        # it is padding indexes in state_repr and will result in zero embeddings\n",
    "\n",
    "    # 候选集合为x个有评分的items和x和无评分的items，推荐item个数等同于rating>3的item个数\n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.viewed_items = []\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\n",
    "        self.good_items = np.argwhere(self.matrix[self.user_id] > 3)[:, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.num_good = len(self.good_items)\n",
    "\n",
    "        # self.available_items = np.arange(9066)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "\n",
    "        self.available_items = np.zeros(self.num_rele*2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "\n",
    "        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :])\n",
    "    \n",
    "    def step(self, action, action_emb=None, buffer=None):\n",
    "        initial_user = self.user_id\n",
    "        initial_memory = self.memory[[initial_user], :]\n",
    "\n",
    "        # 通过rating计算reward（按照原文）\n",
    "        if to_np(action) not in self.related_items:\n",
    "            rating = 0\n",
    "            reward = 0\n",
    "        else:\n",
    "            rating = self.matrix[self.user_id, to_np(action)[0]]\n",
    "            reward = (rating - 3) / 2\n",
    "\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "\n",
    "        # rating>3才更新状态\n",
    "        if reward >0:\n",
    "            if len(action) == 1:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n",
    "            else:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action[0]]\n",
    "\n",
    "        # 推荐商品个数 = rating>3的商品个数\n",
    "        if len(self.viewed_items) == self.num_good:\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "\n",
    "        # 向replay buffer存储SARS\n",
    "        if buffer is not None:\n",
    "            buffer.push(np.array([initial_user]), np.array(initial_memory), to_np(action_emb)[0], \n",
    "                        np.array([reward]), np.array([self.user_id]), self.memory[[self.user_id], :], np.array([done]))\n",
    "\n",
    "        return torch.tensor([self.user_id]), torch.tensor(self.memory[[self.user_id], :]), reward, rating, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8nNg-3ixZ_"
   },
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall model\n",
    "\n",
    "<img src=\"img/full_model.png\" width=\"500\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Actor_DRR(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "\n",
    "    # 只在available items中选择最接近的item，否则大量reward=0，不利于训练\n",
    "    def get_action(self, user, memory, state_repr, \n",
    "                   action_emb,\n",
    "                   items=torch.tensor([i for i in range(item_num)]),\n",
    "                   return_scores=False\n",
    "                  ):\n",
    "        # state没有作用？\n",
    "        state = state_repr(user, memory)\n",
    "        scores = torch.bmm(state_repr.item_embeddings(items).unsqueeze(0), \n",
    "                         action_emb.T.unsqueeze(0)).squeeze(0)\n",
    "        if return_scores:\n",
    "            return scores, torch.gather(items, 0, scores.argmax(0))\n",
    "        else:\n",
    "            return torch.gather(items, 0, scores.argmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Critic_DRR(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_repr_dim + action_emb_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State representation\n",
    "\n",
    "<img src=\"img/state_representation.png\" width=\"350\" height=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        # item需要padding补齐\n",
    "        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\n",
    "        self.drr_ave = torch.nn.Conv1d(in_channels=params['N'], out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.initialize()\n",
    "            \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        self.item_embeddings.weight.data[-1].zero_()\n",
    "        nn.init.uniform_(self.drr_ave.weight)\n",
    "        self.drr_ave.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user, memory):\n",
    "        user_embedding = self.user_embeddings(user.long())\n",
    "\n",
    "        item_embeddings = self.item_embeddings(memory.long())\n",
    "        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we take 1 positive and 99 sampled negatives items per batch, select 10 items with best scores and calculate hit_rate@10 and nDCG@10.\n",
    "During training we choose user 6039 and track `hit` and `dcg` only for him (for evaluation speed). Final scores was computed on the whole test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load pre-trained memory\n",
    "with open('logs/memory.pickle', 'rb') as f:\n",
    "    test_memory = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return dcg_at_k(r, k) / dcg_max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def count_greater_than_x(list, x):\n",
    "    count = 0\n",
    "    for element in list:\n",
    "        if element > x:\n",
    "            count += 1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(net, state_representation, training_env_memory):\n",
    "    test_env = Env(test_matrix)\n",
    "    test_env.memory = training_env_memory.copy()\n",
    "    ndcg_at_10_list = []\n",
    "    precision_at_10_list = []\n",
    "    for user in range(user_num):\n",
    "        user, memory = test_env.reset(user)\n",
    "        if test_env.num_rele < 10:\n",
    "            continue\n",
    "        else:\n",
    "            rating_list = []\n",
    "            for t in range(10):\n",
    "                action_emb = net(state_repr(user, memory))\n",
    "                scores, action = net.get_action(\n",
    "                    user,\n",
    "                    torch.tensor(test_env.memory[to_np(user).astype(int), :]),\n",
    "                    state_representation,\n",
    "                    action_emb,\n",
    "                    torch.tensor([item for item in test_env.related_items\n",
    "                    if item not in test_env.viewed_items]).long(),\n",
    "                    return_scores=True\n",
    "                )\n",
    "                user, memory, reward, rating, done = test_env.step(action)\n",
    "                rating_list.append(rating)\n",
    "            ndcg_at_10 = ndcg_at_k(rating_list,10)\n",
    "            ndcg_at_10_list.append(ndcg_at_10)\n",
    "            precision_at_10_list.append(count_greater_than_x(rating_list,3)/10)\n",
    "    return np.mean(precision_at_10_list), np.mean(ndcg_at_10_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({True: 780, False: 163})"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(test_matrix.tocsr().getnnz(1)>=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(332),\n tensor([[ 0.1853,  0.2085, -0.1968,  0.2282,  0.1742,  0.0438,  0.1719,  0.0227]]))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user, memory = train_env.reset(669)\n",
    "state = state_repr(user, memory)\n",
    "get_action(state, value_net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_action(state, net):\n",
    "    Q_list = []\n",
    "    with torch.no_grad():\n",
    "        candidate_items = torch.tensor(\n",
    "                [item for item in train_env.available_items\n",
    "                if item not in train_env.viewed_items]\n",
    "            ).long()\n",
    "        action_embs = state_repr.item_embeddings(candidate_items)\n",
    "        for action_emb in action_embs:\n",
    "            Q_list.append(net(state,action_emb.unsqueeze(0))[0][0])\n",
    "    max_index = Q_list.index(max(Q_list))\n",
    "    index = candidate_items[max_index]\n",
    "    return index,state_repr.item_embeddings(index).detach().unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "value_net  = Critic_DRR(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim'])\n",
    "replay_buffer = Prioritized_Buffer(params['buffer_size'])\n",
    "\n",
    "target_value_net  = Critic_DRR(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim'])\n",
    "target_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = Ranger(value_net.parameters(),  lr=params['value_lr'], \n",
    "                          weight_decay=params['value_decay'])\n",
    "policy_optimizer = Ranger(policy_net.parameters(), lr=params['policy_lr'], \n",
    "                          weight_decay=params['policy_decay'])\n",
    "state_repr_optimizer = Ranger(state_repr.parameters(), lr=params['state_repr_lr'], \n",
    "                              weight_decay=params['state_repr_decay'])\n",
    "\n",
    "writer = SummaryWriter(log_dir=params['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ddpg_update(training_env,\n",
    "                step=0,\n",
    "                batch_size=params['batch_size'], \n",
    "                gamma=params['gamma'],\n",
    "                min_value=params['min_value'],\n",
    "                max_value=params['max_value'],\n",
    "                soft_tau=params['soft_tau'],\n",
    "               ):\n",
    "    beta = get_beta(step)\n",
    "    user, memory, action, reward, next_user, next_memory, done = replay_buffer.sample(batch_size, beta)\n",
    "    user        = torch.FloatTensor(user)\n",
    "    memory      = torch.FloatTensor(memory)\n",
    "    action      = torch.FloatTensor(action)\n",
    "    reward      = torch.FloatTensor(reward)\n",
    "    next_user   = torch.FloatTensor(next_user)\n",
    "    next_memory = torch.FloatTensor(next_memory)\n",
    "    done = torch.FloatTensor(done)\n",
    "    \n",
    "    state       = state_repr(user, memory)\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "    \n",
    "    next_state     = state_repr(next_user, next_memory)\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "    \n",
    "    state_repr_optimizer.zero_grad()\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    value_optimizer.step()\n",
    "    state_repr_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "                )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    # writer.add_histogram('value', value, step)\n",
    "    # writer.add_histogram('target_value', target_value, step)\n",
    "    # writer.add_histogram('expected_value', expected_value, step)\n",
    "    # writer.add_histogram('policy_loss', policy_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/474 [00:00<?, ?it/s][W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/var/folders/0w/5fjhh0vx7rbg6mmk_ld1nfvr0000gn/T/ipykernel_21671/3694430785.py:45: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n",
      "/var/folders/0w/5fjhh0vx7rbg6mmk_ld1nfvr0000gn/T/ipykernel_21671/4070442190.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525474122/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  action      = torch.FloatTensor(action)\n",
      "/Users/xuxiaoan/opt/anaconda3/envs/recnn/lib/python3.8/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525474122/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "  7%|▋         | 33/474 [00:03<00:41, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5571428571428572 0.9183162658071427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 69/474 [00:11<02:41,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5539083557951483 0.9083497793239135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 102/474 [00:16<02:59,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5601078167115904 0.917235120563439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 136/474 [00:22<02:03,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5679245283018868 0.9268982483770754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 169/474 [00:28<02:39,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5892183288409704 0.9349636798634416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 199/474 [00:34<02:17,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5916442048517521 0.9354313579503668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 232/474 [00:38<00:23, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5924528301886792 0.9357479887414659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 269/474 [00:46<01:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5943396226415094 0.9347814090995712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 301/474 [00:51<01:04,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5916442048517521 0.9321851309421332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 334/474 [00:57<00:54,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5892183288409704 0.9299036872048936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 375/474 [01:03<00:33,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5902964959568733 0.9302711648975932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 412/474 [01:09<00:18,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5905660377358491 0.9308571338852926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 448/474 [01:14<00:09,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595956873315364 0.9336544514424988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [01:17<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(15)\n",
    "train_env = Env(train_matrix)\n",
    "precisions, ndcgs = [], []\n",
    "step, best_step = 0, 0\n",
    "users = np.random.permutation(appropriate_users)\n",
    "ou_noise = OUNoise(params['embedding_dim'], decay_period=10)\n",
    "\n",
    "for epoch in range(params[\"max_epoch\"]):\n",
    "    for u in tqdm.tqdm(users):\n",
    "        user, memory = train_env.reset(u)\n",
    "        if params['ou_noise']:\n",
    "            ou_noise.reset()\n",
    "        for t in range(train_env.num_good):\n",
    "            action_emb = policy_net(state_repr(user, memory))\n",
    "            if params['ou_noise']:\n",
    "                action_emb = ou_noise.get_action(action_emb.detach().cpu().numpy()[0], t)\n",
    "            action = policy_net.get_action(\n",
    "                user,\n",
    "                torch.tensor(train_env.memory[to_np(user).astype(int), :]),\n",
    "                state_repr,\n",
    "                action_emb,\n",
    "                torch.tensor(\n",
    "                    [item for item in train_env.available_items\n",
    "                    if item not in train_env.viewed_items]\n",
    "                ).long()\n",
    "            )\n",
    "\n",
    "            # 返回S'R，并将transition传入replay buffer\n",
    "            user, memory, reward, rating, done = train_env.step(\n",
    "                action,\n",
    "                action_emb,\n",
    "                buffer=replay_buffer\n",
    "            )\n",
    "\n",
    "            if len(replay_buffer) > params['batch_size']:\n",
    "                ddpg_update(train_env, step=step)\n",
    "\n",
    "            # 每1000个step保存最优参数（使得ndcg@10最大）（使用同一个test_memory）\n",
    "            if step % 1000 == 0 and step > 0:\n",
    "                precision, ndcg = run_evaluation(policy_net, state_repr, train_env.memory)\n",
    "                # writer.add_scalar('precision', precision, step)\n",
    "                # writer.add_scalar('ndcg', ndcg, step)\n",
    "                precisions.append(precision)\n",
    "                ndcgs.append(ndcg)\n",
    "                print(precision,ndcg)\n",
    "                if np.mean(np.array([ndcg]) - np.array(ndcgs[best_step])) > 0:\n",
    "                    # best_step是当前step在ndcgs中的index，ndcgs的元素为从1000开始\n",
    "                    best_step = step // 1000 - 1\n",
    "                    torch.save(policy_net.state_dict(), params['log_dir'] + 'best_policy_net.pth')\n",
    "                    torch.save(value_net.state_dict(), params['log_dir'] + 'best_value_net.pth')\n",
    "                    torch.save(state_repr.state_dict(), params['log_dir'] + 'best_state_repr.pth')\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net_final.pth')\n",
    "torch.save(value_net.state_dict(), params['log_dir'] + 'value_net_final.pth')\n",
    "torch.save(state_repr.state_dict(), params['log_dir'] + 'state_repr_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need memory for validation, so it's better to save it and not wait next time \n",
    "with open('logs/memory.pickle', 'wb') as f:\n",
    "    pickle.dump(train_env.memory, f)\n",
    "    \n",
    "with open('logs/memory.pickle', 'rb') as f:\n",
    "    memory = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# load pre-trained memory\n",
    "with open('logs/memory2.pickle', 'rb') as f:\n",
    "    memory = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and logs are stored in [this folder](https://drive.google.com/drive/folders/1hsGjh8oHN4uyCmp_wtAyVPTR76ylVVgH?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0w/5fjhh0vx7rbg6mmk_ld1nfvr0000gn/T/ipykernel_21671/3694430785.py:45: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate:  0.592722371967655 dcg:  0.9362027842132209\n"
     ]
    }
   ],
   "source": [
    "no_ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_state_repr.load_state_dict(torch.load('logs/final/' + 'best_state_repr.pth'))\n",
    "no_ou_policy_net.load_state_dict(torch.load('logs/final/' + 'best_policy_net.pth'))\n",
    "\n",
    "hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, train_env.memory)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0w/5fjhh0vx7rbg6mmk_ld1nfvr0000gn/T/ipykernel_33216/4137504441.py:33: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate:  0.502304510193194 dcg:  0.2798106994373345\n"
     ]
    }
   ],
   "source": [
    "ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "ou_policy_net = Actor_DRR(params['embedding_dim'], params['hidden_dim'])\n",
    "ou_state_repr.load_state_dict(torch.load('logs/ou_noise_04/' + 'best_state_repr.pth'))\n",
    "ou_policy_net.load_state_dict(torch.load('logs/ou_noise_04/' + 'best_policy_net.pth'))\n",
    "\n",
    "hit, dcg = run_evaluation(ou_policy_net, ou_state_repr, test_memory)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of trained agents behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose random user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5983\n"
     ]
    }
   ],
   "source": [
    "random_user = np.random.randint(user_num)\n",
    "print(random_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                               name  \\\n44      45                                  To Die For (1995)   \n64      65                                    Bio-Dome (1996)   \n131    133                                   Nueba Yol (1995)   \n639    644                               Happy Weekend (1996)   \n706    715  Horseman on the Roof, The (Hussard sur le toit...   \n979    991                             Michael Collins (1996)   \n989   1002                              Ed's Next Move (1996)   \n1222  1242                                       Glory (1989)   \n\n                 genre  \n44        Comedy|Drama  \n64              Comedy  \n131       Comedy|Drama  \n639             Comedy  \n706              Drama  \n979          Drama|War  \n989             Comedy  \n1222  Action|Drama|War  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>44</th>\n      <td>45</td>\n      <td>To Die For (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>65</td>\n      <td>Bio-Dome (1996)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>133</td>\n      <td>Nueba Yol (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>644</td>\n      <td>Happy Weekend (1996)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>706</th>\n      <td>715</td>\n      <td>Horseman on the Roof, The (Hussard sur le toit...</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>979</th>\n      <td>991</td>\n      <td>Michael Collins (1996)</td>\n      <td>Drama|War</td>\n    </tr>\n    <tr>\n      <th>989</th>\n      <td>1002</td>\n      <td>Ed's Next Move (1996)</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>1222</th>\n      <td>1242</td>\n      <td>Glory (1989)</td>\n      <td>Action|Drama|War</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies = pd.read_csv('/Users/xuxiaoan/Downloads/recsys-rl-master/data/movies.dat', sep='::', header=None, engine='python', names=['id', 'name', 'genre'], encoding=\"latin-1\")\n",
    "# in the code numeration starts with 0\n",
    "display(movies[movies['id'].isin(np.argwhere(test_matrix[random_user] > 0)[:, 1] + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can recommend \"Nixon\" and \"Love Serenade\" and see next 3 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([132]), tensor([714]), tensor([64])]\n",
      "[tensor([132]), tensor([64]), tensor([714])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0w/5fjhh0vx7rbg6mmk_ld1nfvr0000gn/T/ipykernel_33216/4137504441.py:33: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [action]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for model, state_representation in zip([ou_policy_net, no_ou_policy_net], [ou_state_repr, no_ou_state_repr]):\n",
    "    example_env = Env(test_matrix)\n",
    "    user, memory = example_env.reset(random_user)\n",
    "\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([44])) # 13\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([1001])) # 1584\n",
    "    preds = []\n",
    "    for _ in range(3):\n",
    "        action_emb = model(state_representation(user, memory))\n",
    "        action = model.get_action(\n",
    "            user, \n",
    "            torch.tensor(example_env.memory[to_np(user).astype(int), :]), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            torch.tensor(\n",
    "               [item for item in example_env.available_items\n",
    "               if item not in example_env.viewed_items]\n",
    "            ).long()\n",
    "        )\n",
    "        user, memory, reward, _ = example_env.step(action)\n",
    "        preds.append(action)\n",
    "\n",
    "    predictions.append(preds)\n",
    "\n",
    "print(predictions[0])\n",
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained with OU noise recommended related `Comedy` and `Documentary` after that switch recommendations to nonrelated `Crime|Film-Noir|Thriller`.\n",
    "Model trained without OU noise recommended `Comedy|Drama`, `Children's|Comedy`, `Drama` (two of them are related).\n",
    "\n",
    "Both models seems to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=img/learning_curve.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs are consistent with expectations. Adding noise increase metrics (std=0.4 performs the best, after 0.6 model starts to degrade)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s-iWfb5TixZc",
    "baIIDXdAixaH"
   ],
   "name": "pytorch.pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "recnn",
   "language": "python",
   "display_name": "recnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
